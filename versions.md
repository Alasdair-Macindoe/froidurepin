# Introduction
Please refer to `data.md` for the input corresponding to each number.
Additionally to `runtimes.md` for the runtime of each version.  

`Version 2.1` is an exact implementation from the paper.

Then let Y = the set of reduced words at a specific stage, and A = the generators for this semigroup.

Finally note that 'Concurren(t/cy) Processing Unit' is either a thread, process or task.

# Version 1 - One-Dimensional Array Data Structure with No Concurrency and Locking

In this version there is a one-dimensional array that holds the entire graph, and there are indexes stored that tell us the front and the end of the current frontier of the graph.

Using the running example we get A = [a, b]. After we initialise our data structure we get Y = [a, b] with our start = 1, and end = 2.

After we performing our multiplications to produce our Cayley table we add [c, d] this will give us Y = [a, b, c, d] with start = 3 and end = 4. This is where the calculations would end and our Y would contain a datastructure that allows us to create the graph. For example if we take Y[1] our data structure will include right[2] (this means Y[1] (a) times A[2] (b) which will give us a). In effect c and d are the forefront of our graph.

### Note

Because of how poorly I thought this out this means process queues cannot be done concurrently. This was improved upon in version 2.

## Apply Generators

The queue generated by `ApplyGenerators` is a 2-D array (technically ApplyGenerators amends in place an already created 2-D array). Each bucket is given its own array inside the main array (hence making it 2-D) and each concurrent unit of processing gets its own bucket (thus there are as many buckets as there are concurrent processing) meaning that there is no need for locking here since there cannot be any problems.

If we consider at the start our Queue Q = [[], []] which will have two units of concurrency. At the end of Apply Generators. This could become: [ [c, d], [d]]. AT this point c, d are not values but full datastructures (although no left Cayley graph created at this point).

## Merge Queue

`MergeQueues` takes all the potentially new values from `ApplyGenerators` and merges them into our reduced words collection Y if they are new, and discards them otherwise.

If we have Y = [a, b] and our Queues Q = [ [c, d], [d]] then at the end of our this method our Y = [a, b, c, d].

## Develop Left

Takes the new Y produced from MergeQueues and creates the left Cayley graph for any values that needs them. Nothing exceptionally special. No multiplications happen at this stage.

## Jobs
For this version the standard number of jobs (effectively how many fragements we take) is one per generator.

# Version 1.1.x

All of these Versions will use a bucket per generator in ApplyGenerators output and will split fragment the known search space based upon two conditions: firstly how large our frontier is and how many jobs we have (which by default is the number of generators we are given).

## Version 1.1.1 - Tasks & Locking

This version is an extension of Version 1. It has concurrency introduced using
HPC-GAP's Task system and uses locking to allow us to maintain a 1-D array for our Y set (the set of reduced words). This is done using HPC-GAP's atomic types.

# Version 2 - Paper Implementation

This implementation is identical to the one given in the paper. Version 2.0 is the non-concurrent implementation and Version 2.1 uses HPC-GAP's built in `tasks` for concurrency.

## Data Structures and Supplementary Functions

### Bucket Function (Hash Function)

Unfortunately HPC-GAP does not seem to have a built in hash function for Transformations (which is what I am traditionally working with) as such I had to design my own for Transformations. It is very important to note that the bucket function **must** be deterministic.

### Fragment

The paper defines a fragment. From a computational perspective a fragment is a collection of words and a value K which is used as a counter to the first unread (by Apply Generators) word. For example if Apply Generators had read the first and second word then K would be 3. Words are stored as an List structure and access control is not needed on them.

### Fragments

All of the fragments are stored in a list which makes it easy to assign a fragment to a task because each task has a job number which is the index into the list.

### Word

A Word is the concatenation of two values (or a generator value). It contains two attributes: firstly the bucket number it should be placed in (in Apply Queues) and secondly the `wordRec` which is the word's value, suffix, prefix, and all other information required which is full described in `Word Entry`.

### Word Entry

A Word Entry defines the state of a word. It defines its value, what words create it, what happens when it is multiplied (left and right) by a generator and its length. This means we substitute a few of the functions defined in the paper for memory lookups. We are of the opinion that this increases the readability of the code.

### Reduced Words

Each reduced word is stored in a list within the fragment. This is not a perfect solution, but HPC-GAP has no hash function for `Transformations`. 

### CreateQueues

CreateQueues method creates the buckets that each generated word (from Apply Generators) will be placed into. This needs to be equal to the number of jobs that there will be.

## Main Algorithm

### Apply Generators

Each instance of Apply Generators created (one per task) deals with a specific fragment of reduced words (`Y` is the set of all fragments and internally `Yj` is used for the jth fragment). It uses the K value defined within that fragment to iterate until we have exceeded the size of the fragment (note that at this stage we are *never* increasing the size of fragments) and that the word is the length that we are currently iterating over. We do not require a lock on the fragment `Yj` because it is only being used in one thread; the thread with the `jth` job assigned to it.

We then require the Kth word for that fragment (internally called `YjKj`) which you may notice we do not need to lock either because for each job (which gets its own task) the iteration is linear across the words which means each thread is accessing at most one word in its fragment's words at any time. For every generator we perform a series of instructions. Firstly we check to see if its suffix's right multiple by this generator is reduced or not and if it is not reduced then we reduce it.

Secondly we find the right multiple of this word with that generator and see if it exists within the fragment we are assigned to. Note that no locking is required at this point since we are not ever editing anything within the set of reduced words, if we do amend a word it is specific to that task which means no other job will be trying to access it.
If the word exists then its right multiple is updated, otherwise we create a new word with that value (which gives it a bucket number) and add it to that bucket and added to that jobs queue for Process Queues.
Further note that a word *may* exist in another fragment this means that not all values placed into the queue will actually be unique - this is done to avoid any locking.

### Process Queues

In order to remove the requirement for locking the implementation makes each `ProcessQueues` loop over every word in every queue (this is read only because we do not update the words inside `ProcessQueues`) and then perform the merging steps on words that have the bucket entry corresponding to its job number (this means each `ProcessQueues` instances deals with a different bucket meaning no concurrency issues arise). This works because each word (until the inner part) is read only and since each word can only have one bucket (and each instance has a different bucket) there can be no race conditions when we merge the word (no other task has this word and no other task is using this bucket). Additionally notice that the bucketing function is deterministic this means that any identical words are put in the same bucket even if they have different queue numbers meaning that two tasks will never attempt to add the same word.
For clarity: We are adding any unique word (from all fragments) to a specific fragment which is what ever task has the bucket that word is assigned to.

### Develop Left

The paper does not define `DevelopLeft` but we believed the code looked neater if it was abstracted to its own function. `DevelopLeft` develops the left Caley graph for each newly added word as well as performing any reductions of older words. Each task of `DevelopLeft` deals with exactly one of the fragments (which at this stage all have unique words). Occasionally words in other fragments may need to accessed but it is read only in this task and the part of the word we care about (the prefix) is never updated once set allowing us to do this safely.
